{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "keypoint_CNN.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "53320d6fe6eb4cedad0a6d3ff9529650": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_41ea44c847b84e2f8c397b789d6bd632",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_c1bf97d540114d5eb882832eb42c66f7",
              "IPY_MODEL_1782bc949b9b42bcb06dc7585813c4a0"
            ]
          }
        },
        "41ea44c847b84e2f8c397b789d6bd632": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "c1bf97d540114d5eb882832eb42c66f7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_48344a87aff147ceb1fcf425e9e48b3c",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 237034793,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 237034793,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_460aeed0a92540b8aa32890824bc6e46"
          }
        },
        "1782bc949b9b42bcb06dc7585813c4a0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_55d1bed77daa4b6d86b18e804870c5bb",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 226M/226M [00:06&lt;00:00, 39.0MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_304507900c2840578fb3bb915ae60d9f"
          }
        },
        "48344a87aff147ceb1fcf425e9e48b3c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "460aeed0a92540b8aa32890824bc6e46": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "55d1bed77daa4b6d86b18e804870c5bb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "304507900c2840578fb3bb915ae60d9f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "JQ9iLRl50CuJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "83b18353-c40b-46e8-fee6-bedd4b77747a"
      },
      "source": [
        "!git clone https://github.com/pytorch/vision.git\n",
        "!cp /content/vision/references/detection/*.py /content"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'vision'...\n",
            "remote: Enumerating objects: 27, done.\u001b[K\n",
            "remote: Counting objects:   3% (1/27)\u001b[K\rremote: Counting objects:   7% (2/27)\u001b[K\rremote: Counting objects:  11% (3/27)\u001b[K\rremote: Counting objects:  14% (4/27)\u001b[K\rremote: Counting objects:  18% (5/27)\u001b[K\rremote: Counting objects:  22% (6/27)\u001b[K\rremote: Counting objects:  25% (7/27)\u001b[K\rremote: Counting objects:  29% (8/27)\u001b[K\rremote: Counting objects:  33% (9/27)\u001b[K\rremote: Counting objects:  37% (10/27)\u001b[K\rremote: Counting objects:  40% (11/27)\u001b[K\rremote: Counting objects:  44% (12/27)\u001b[K\rremote: Counting objects:  48% (13/27)\u001b[K\rremote: Counting objects:  51% (14/27)\u001b[K\rremote: Counting objects:  55% (15/27)\u001b[K\rremote: Counting objects:  59% (16/27)\u001b[K\rremote: Counting objects:  62% (17/27)\u001b[K\rremote: Counting objects:  66% (18/27)\u001b[K\rremote: Counting objects:  70% (19/27)\u001b[K\rremote: Counting objects:  74% (20/27)\u001b[K\rremote: Counting objects:  77% (21/27)\u001b[K\rremote: Counting objects:  81% (22/27)\u001b[K\rremote: Counting objects:  85% (23/27)\u001b[K\rremote: Counting objects:  88% (24/27)\u001b[K\rremote: Counting objects:  92% (25/27)\u001b[K\rremote: Counting objects:  96% (26/27)\u001b[K\rremote: Counting objects: 100% (27/27)\u001b[K\rremote: Counting objects: 100% (27/27), done.\u001b[K\n",
            "remote: Compressing objects:   3% (1/27)\u001b[K\rremote: Compressing objects:   7% (2/27)\u001b[K\rremote: Compressing objects:  11% (3/27)\u001b[K\rremote: Compressing objects:  14% (4/27)\u001b[K\rremote: Compressing objects:  18% (5/27)\u001b[K\rremote: Compressing objects:  22% (6/27)\u001b[K\rremote: Compressing objects:  25% (7/27)\u001b[K\rremote: Compressing objects:  29% (8/27)\u001b[K\rremote: Compressing objects:  33% (9/27)\u001b[K\rremote: Compressing objects:  37% (10/27)\u001b[K\rremote: Compressing objects:  40% (11/27)\u001b[K\rremote: Compressing objects:  44% (12/27)\u001b[K\rremote: Compressing objects:  48% (13/27)\u001b[K\rremote: Compressing objects:  51% (14/27)\u001b[K\rremote: Compressing objects:  55% (15/27)\u001b[K\rremote: Compressing objects:  59% (16/27)\u001b[K\rremote: Compressing objects:  62% (17/27)\u001b[K\rremote: Compressing objects:  66% (18/27)\u001b[K\rremote: Compressing objects:  70% (19/27)\u001b[K\rremote: Compressing objects:  74% (20/27)\u001b[K\rremote: Compressing objects:  77% (21/27)\u001b[K\rremote: Compressing objects:  81% (22/27)\u001b[K\rremote: Compressing objects:  85% (23/27)\u001b[K\rremote: Compressing objects:  88% (24/27)\u001b[K\rremote: Compressing objects:  92% (25/27)\u001b[K\rremote: Compressing objects:  96% (26/27)\u001b[K\rremote: Compressing objects: 100% (27/27)\u001b[K\rremote: Compressing objects: 100% (27/27), done.\u001b[K\n",
            "Receiving objects:   0% (1/9483)   \rReceiving objects:   1% (95/9483)   \rReceiving objects:   2% (190/9483)   \rReceiving objects:   3% (285/9483)   \rReceiving objects:   4% (380/9483)   \rReceiving objects:   5% (475/9483)   \rReceiving objects:   6% (569/9483)   \rReceiving objects:   7% (664/9483)   \rReceiving objects:   8% (759/9483)   \rReceiving objects:   9% (854/9483)   \rReceiving objects:  10% (949/9483)   \rReceiving objects:  11% (1044/9483)   \rReceiving objects:  12% (1138/9483)   \rReceiving objects:  13% (1233/9483)   \rReceiving objects:  14% (1328/9483)   \rReceiving objects:  15% (1423/9483)   \rReceiving objects:  16% (1518/9483)   \rReceiving objects:  17% (1613/9483)   \rReceiving objects:  18% (1707/9483)   \rReceiving objects:  19% (1802/9483)   \rReceiving objects:  20% (1897/9483)   \rReceiving objects:  21% (1992/9483)   \rReceiving objects:  22% (2087/9483)   \rReceiving objects:  23% (2182/9483)   \rReceiving objects:  24% (2276/9483)   \rReceiving objects:  25% (2371/9483)   \rReceiving objects:  26% (2466/9483)   \rReceiving objects:  27% (2561/9483)   \rReceiving objects:  28% (2656/9483)   \rReceiving objects:  29% (2751/9483)   \rReceiving objects:  30% (2845/9483)   \rReceiving objects:  31% (2940/9483)   \rReceiving objects:  32% (3035/9483)   \rReceiving objects:  33% (3130/9483)   \rReceiving objects:  34% (3225/9483)   \rReceiving objects:  35% (3320/9483)   \rReceiving objects:  36% (3414/9483)   \rReceiving objects:  37% (3509/9483)   \rReceiving objects:  38% (3604/9483)   \rReceiving objects:  39% (3699/9483)   \rReceiving objects:  40% (3794/9483)   \rReceiving objects:  41% (3889/9483)   \rReceiving objects:  42% (3983/9483)   \rReceiving objects:  43% (4078/9483)   \rReceiving objects:  44% (4173/9483)   \rReceiving objects:  45% (4268/9483)   \rReceiving objects:  46% (4363/9483)   \rReceiving objects:  47% (4458/9483)   \rReceiving objects:  48% (4552/9483)   \rReceiving objects:  49% (4647/9483)   \rReceiving objects:  50% (4742/9483)   \rReceiving objects:  51% (4837/9483)   \rReceiving objects:  52% (4932/9483)   \rReceiving objects:  53% (5026/9483)   \rReceiving objects:  54% (5121/9483)   \rReceiving objects:  55% (5216/9483)   \rReceiving objects:  56% (5311/9483)   \rReceiving objects:  57% (5406/9483)   \rReceiving objects:  58% (5501/9483)   \rReceiving objects:  59% (5595/9483)   \rReceiving objects:  60% (5690/9483)   \rReceiving objects:  61% (5785/9483)   \rReceiving objects:  62% (5880/9483)   \rReceiving objects:  63% (5975/9483)   \rReceiving objects:  64% (6070/9483)   \rReceiving objects:  65% (6164/9483)   \rReceiving objects:  66% (6259/9483)   \rReceiving objects:  67% (6354/9483)   \rReceiving objects:  68% (6449/9483)   \rReceiving objects:  69% (6544/9483)   \rReceiving objects:  70% (6639/9483)   \rReceiving objects:  71% (6733/9483)   \rReceiving objects:  72% (6828/9483)   \rReceiving objects:  73% (6923/9483)   \rReceiving objects:  74% (7018/9483), 10.68 MiB | 21.19 MiB/s   \rReceiving objects:  75% (7113/9483), 10.68 MiB | 21.19 MiB/s   \rReceiving objects:  76% (7208/9483), 10.68 MiB | 21.19 MiB/s   \rReceiving objects:  77% (7302/9483), 10.68 MiB | 21.19 MiB/s   \rReceiving objects:  78% (7397/9483), 10.68 MiB | 21.19 MiB/s   \rReceiving objects:  79% (7492/9483), 10.68 MiB | 21.19 MiB/s   \rReceiving objects:  80% (7587/9483), 10.68 MiB | 21.19 MiB/s   \rReceiving objects:  81% (7682/9483), 10.68 MiB | 21.19 MiB/s   \rReceiving objects:  82% (7777/9483), 10.68 MiB | 21.19 MiB/s   \rReceiving objects:  83% (7871/9483), 10.68 MiB | 21.19 MiB/s   \rReceiving objects:  84% (7966/9483), 10.68 MiB | 21.19 MiB/s   \rReceiving objects:  85% (8061/9483), 10.68 MiB | 21.19 MiB/s   \rReceiving objects:  86% (8156/9483), 10.68 MiB | 21.19 MiB/s   \rReceiving objects:  87% (8251/9483), 10.68 MiB | 21.19 MiB/s   \rReceiving objects:  88% (8346/9483), 10.68 MiB | 21.19 MiB/s   \rReceiving objects:  89% (8440/9483), 10.68 MiB | 21.19 MiB/s   \rReceiving objects:  90% (8535/9483), 10.68 MiB | 21.19 MiB/s   \rReceiving objects:  91% (8630/9483), 10.68 MiB | 21.19 MiB/s   \rReceiving objects:  92% (8725/9483), 10.68 MiB | 21.19 MiB/s   \rReceiving objects:  93% (8820/9483), 10.68 MiB | 21.19 MiB/s   \rReceiving objects:  94% (8915/9483), 10.68 MiB | 21.19 MiB/s   \rReceiving objects:  95% (9009/9483), 10.68 MiB | 21.19 MiB/s   \rremote: Total 9483 (delta 9), reused 5 (delta 0), pack-reused 9456\u001b[K\n",
            "Receiving objects:  96% (9104/9483), 10.68 MiB | 21.19 MiB/s   \rReceiving objects:  97% (9199/9483), 10.68 MiB | 21.19 MiB/s   \rReceiving objects:  98% (9294/9483), 10.68 MiB | 21.19 MiB/s   \rReceiving objects:  99% (9389/9483), 10.68 MiB | 21.19 MiB/s   \rReceiving objects: 100% (9483/9483), 10.68 MiB | 21.19 MiB/s   \rReceiving objects: 100% (9483/9483), 11.34 MiB | 18.14 MiB/s, done.\n",
            "Resolving deltas:   0% (0/6573)   \rResolving deltas:   1% (66/6573)   \rResolving deltas:   2% (132/6573)   \rResolving deltas:   3% (207/6573)   \rResolving deltas:   4% (267/6573)   \rResolving deltas:   5% (337/6573)   \rResolving deltas:   6% (401/6573)   \rResolving deltas:   7% (469/6573)   \rResolving deltas:   8% (545/6573)   \rResolving deltas:   9% (592/6573)   \rResolving deltas:  10% (661/6573)   \rResolving deltas:  11% (724/6573)   \rResolving deltas:  12% (815/6573)   \rResolving deltas:  13% (856/6573)   \rResolving deltas:  14% (935/6573)   \rResolving deltas:  15% (992/6573)   \rResolving deltas:  16% (1097/6573)   \rResolving deltas:  17% (1120/6573)   \rResolving deltas:  18% (1187/6573)   \rResolving deltas:  19% (1258/6573)   \rResolving deltas:  20% (1315/6573)   \rResolving deltas:  21% (1386/6573)   \rResolving deltas:  22% (1452/6573)   \rResolving deltas:  23% (1516/6573)   \rResolving deltas:  24% (1579/6573)   \rResolving deltas:  25% (1644/6573)   \rResolving deltas:  26% (1727/6573)   \rResolving deltas:  27% (1778/6573)   \rResolving deltas:  28% (1853/6573)   \rResolving deltas:  29% (1918/6573)   \rResolving deltas:  30% (1984/6573)   \rResolving deltas:  31% (2047/6573)   \rResolving deltas:  32% (2155/6573)   \rResolving deltas:  33% (2171/6573)   \rResolving deltas:  34% (2254/6573)   \rResolving deltas:  35% (2307/6573)   \rResolving deltas:  37% (2465/6573)   \rResolving deltas:  38% (2498/6573)   \rResolving deltas:  39% (2596/6573)   \rResolving deltas:  40% (2637/6573)   \rResolving deltas:  41% (2696/6573)   \rResolving deltas:  42% (2786/6573)   \rResolving deltas:  44% (2896/6573)   \rResolving deltas:  45% (2966/6573)   \rResolving deltas:  46% (3025/6573)   \rResolving deltas:  47% (3094/6573)   \rResolving deltas:  48% (3164/6573)   \rResolving deltas:  49% (3222/6573)   \rResolving deltas:  50% (3327/6573)   \rResolving deltas:  51% (3353/6573)   \rResolving deltas:  52% (3452/6573)   \rResolving deltas:  53% (3492/6573)   \rResolving deltas:  55% (3661/6573)   \rResolving deltas:  56% (3686/6573)   \rResolving deltas:  57% (3808/6573)   \rResolving deltas:  58% (3834/6573)   \rResolving deltas:  59% (3879/6573)   \rResolving deltas:  60% (3947/6573)   \rResolving deltas:  61% (4024/6573)   \rResolving deltas:  62% (4081/6573)   \rResolving deltas:  63% (4141/6573)   \rResolving deltas:  64% (4214/6573)   \rResolving deltas:  65% (4278/6573)   \rResolving deltas:  66% (4345/6573)   \rResolving deltas:  67% (4405/6573)   \rResolving deltas:  68% (4488/6573)   \rResolving deltas:  69% (4556/6573)   \rResolving deltas:  70% (4607/6573)   \rResolving deltas:  71% (4672/6573)   \rResolving deltas:  72% (4734/6573)   \rResolving deltas:  73% (4801/6573)   \rResolving deltas:  74% (4866/6573)   \rResolving deltas:  75% (4931/6573)   \rResolving deltas:  77% (5081/6573)   \rResolving deltas:  78% (5140/6573)   \rResolving deltas:  79% (5242/6573)   \rResolving deltas:  80% (5268/6573)   \rResolving deltas:  81% (5327/6573)   \rResolving deltas:  82% (5394/6573)   \rResolving deltas:  83% (5482/6573)   \rResolving deltas:  84% (5533/6573)   \rResolving deltas:  85% (5588/6573)   \rResolving deltas:  86% (5656/6573)   \rResolving deltas:  87% (5735/6573)   \rResolving deltas:  88% (5785/6573)   \rResolving deltas:  89% (5865/6573)   \rResolving deltas:  90% (5933/6573)   \rResolving deltas:  91% (5986/6573)   \rResolving deltas:  92% (6050/6573)   \rResolving deltas:  93% (6116/6573)   \rResolving deltas:  94% (6179/6573)   \rResolving deltas:  95% (6271/6573)   \rResolving deltas:  96% (6312/6573)   \rResolving deltas:  97% (6393/6573)   \rResolving deltas:  98% (6442/6573)   \rResolving deltas:  99% (6526/6573)   \rResolving deltas: 100% (6573/6573)   \rResolving deltas: 100% (6573/6573), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9hwSWqU1I0E8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#import your data\n",
        "import json\n",
        "import cv2\n",
        "import glob \n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "image=[]\n",
        "obj=[]\n",
        "  \n",
        "for i,name in enumerate(glob.glob('/content/drive/My Drive/labels2/*.png')): \n",
        "  if os.path.exists(name+\"___objects.json\"):\n",
        "    with open(name+'___objects.json') as f:\n",
        "      obj.append( json.load(f))\n",
        "    image.append(cv2.imread(name))\n",
        "    #obj.append(name+\"___objects.json\")\n",
        "  else:\n",
        "    print(False)\n",
        "  print(i,name) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vjUk5w_QPVM5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#view data\n",
        "import matplotlib.pyplot as plt\n",
        "plt.imshow(cv2.cvtColor(image[0], cv2.COLOR_BGR2RGB))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lfmB4mXDQLfy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#dataloader\n",
        "import torchvision.transforms as T\n",
        "import torch \n",
        "import numpy as np\n",
        "\n",
        "class AppleDataset(object):\n",
        "    def __init__(self, obj,images):\n",
        "        \n",
        "        self.dtype = torch.cuda.FloatTensor if torch.cuda.is_available() else torch.FloatTensor\n",
        "        self.to_tensor = T.ToTensor()\n",
        "        self.device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "        self.obj = obj\n",
        "        self.images=images\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        \n",
        "        vis={'True':1,'False':0}\n",
        "        pts=[]\n",
        "        boxes=[]\n",
        "        for i in self.obj[idx]:\n",
        "          if i['type']=='point':\n",
        "            pts.append(i['x'])\n",
        "            pts.append(i['y'])\n",
        "            pts.append(1)\n",
        "          elif i['type']=='bbox':\n",
        "            boxes.append([i['points']['x1'],i['points']['y1'],i['points']['x2'],i['points']['y2']])\n",
        "\n",
        "         \n",
        "        image =self.images[idx]\n",
        "        #kpts = np.array(self.df.iloc[idx, 1:]).astype(np.float32).reshape([-1, 3]) # [x, y, visibility]\n",
        "        kpts=np.array(pts).astype(np.float32).reshape([1,-1,3])#considering one apple per imageyzyy\n",
        "        boxes=np.array(boxes).astype(np.float32)\n",
        "        h, w = image.shape[:2]\n",
        "        \n",
        "        if w < 640:\n",
        "            image = imutils.resize(image, width=640)\n",
        "        \n",
        "        labels = np.ones((1), dtype=np.int8)\n",
        "        image_id = torch.tensor([idx])\n",
        "        target = {}\n",
        "        target[\"boxes\"] = torch.from_numpy(boxes).type(self.dtype).to(self.device)\n",
        "        target[\"labels\"] = torch.from_numpy(labels).type(torch.int64).to(self.device)\n",
        "        target[\"keypoints\"] = torch.from_numpy(kpts).type(self.dtype).to(self.device)\n",
        "        target[\"image_id\"] = image_id.to(self.device)\n",
        "        target[\"area\"] =(torch.tensor(boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])).to(self.device) \n",
        "        iscrowd = torch.zeros((1,), dtype=torch.int64)#one apple per image\n",
        "        target[\"iscrowd\"] = iscrowd.to(self.device)\n",
        "        img = T.ToPILImage()(image).convert('RGB')\n",
        "        img = self.to_tensor(img).to(self.device)        \n",
        "\n",
        "        return img, target"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FmbfvaMxqili",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "2f0c4dc9-9bfb-435d-eebb-5a6bc3c0c99c"
      },
      "source": [
        "print(torch.cuda.is_available(),\n",
        "torch.cuda.get_device_capability(),\n",
        "torch.cuda.device_count(),\n",
        "torch.cuda.get_device_name(device=None))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "True (3, 7) 1 Tesla K80\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E9kR2rTxWun-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Dataset = AppleDataset(obj, image)\n",
        "\n",
        "l=Dataset.__len__()\n",
        "test_percent=5\n",
        "torch.manual_seed(1)\n",
        "indices = torch.randperm(len(Dataset)).tolist()\n",
        "dataset = torch.utils.data.Subset(Dataset, indices[:-int(np.ceil(l*test_percent/100))])\n",
        "dataset_test = torch.utils.data.Subset(Dataset, indices[int(-np.ceil(l*test_percent/100)):])"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4JvVOoPohYJX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(dataset.__len__(),dataset_test.__len__())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HdXxnpLTbDVF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# define training and validation data loaders\n",
        "import utils\n",
        "data_loader = torch.utils.data.DataLoader(\n",
        "    dataset, batch_size=2, shuffle=True, \n",
        "    collate_fn=utils.collate_fn)\n",
        "\n",
        "data_loader_test = torch.utils.data.DataLoader(\n",
        "    dataset_test, batch_size=(1), shuffle=False, \n",
        "    collate_fn=utils.collate_fn)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nNqwFItPtmTu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "def get_model(num_kpts,train_kptHead=False,train_fpn=True):\n",
        "    is_available = torch.cuda.is_available()\n",
        "    device =torch.device('cuda:0' if is_available else 'cpu')\n",
        "    dtype = torch.cuda.FloatTensor if is_available else torch.FloatTensor\n",
        "    model = torchvision.models.detection.keypointrcnn_resnet50_fpn(pretrained=True)\n",
        "    \n",
        "    for i,param in enumerate(model.parameters()):\n",
        "        param.requires_grad = False\n",
        "        \n",
        "    if train_kptHead!=False:\n",
        "      for i, param in enumerate(model.roi_heads.keypoint_head.parameters()):\n",
        "          if i/2>=model.roi_heads.keypoint_head.__len__()/2-train_kptHead:\n",
        "            param.requires_grad = True\n",
        "\n",
        "    if train_fpn==True:\n",
        "      for param in model.backbone.fpn.parameters():\n",
        "        param.requires_grad = True\n",
        "\n",
        "    out = nn.ConvTranspose2d(512, num_kpts, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
        "    model.roi_heads.keypoint_predictor.kps_score_lowres = out\n",
        "    \n",
        "    return model, device, dtype\n",
        "#model, device, dtype=get_model(2)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QuaxWZrXNp26",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 120,
          "referenced_widgets": [
            "53320d6fe6eb4cedad0a6d3ff9529650",
            "41ea44c847b84e2f8c397b789d6bd632",
            "c1bf97d540114d5eb882832eb42c66f7",
            "1782bc949b9b42bcb06dc7585813c4a0",
            "48344a87aff147ceb1fcf425e9e48b3c",
            "460aeed0a92540b8aa32890824bc6e46",
            "55d1bed77daa4b6d86b18e804870c5bb",
            "304507900c2840578fb3bb915ae60d9f"
          ]
        },
        "outputId": "3545fa33-0052-43bf-f114-61f5a9a9b8fd"
      },
      "source": [
        "#load a pretrained model\n",
        "from google.colab import files\n",
        "model_save_name = 'classifier3.pth'\n",
        "path = F\"/content/drive/My Drive/{model_save_name}\" \n",
        "model, device, dtype=get_model(2,train_kptHead=2,train_fpn=True)\n",
        "model.to(device)\n",
        "model.load_state_dict(torch.load(path))\n",
        "#torch.save(model.state_dict(), path)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/keypointrcnn_resnet50_fpn_coco-fc266e95.pth\" to /root/.cache/torch/hub/checkpoints/keypointrcnn_resnet50_fpn_coco-fc266e95.pth\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "53320d6fe6eb4cedad0a6d3ff9529650",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=237034793.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o3uFhWIgdu14",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#code to plot the results. line joining two keypoints in this case\n",
        "from PIL import Image\n",
        "import numpy\n",
        "from google.colab import files\n",
        "\n",
        "def show_result(data,no_images=50):\n",
        "\n",
        "  figsize=(500, 1280)\n",
        "  columns = 1\n",
        "  rows = 50\n",
        "  fig=plt.figure()\n",
        "  j=0\n",
        "  for i,(image,keypoints) in enumerate(data):\n",
        "   \n",
        "    image=image.cpu().permute(1, 2, 0).numpy()\n",
        "    image=image[:, :, [2, 1, 0]]  \n",
        "    cX1=int(keypoints['keypoints'][0][0][0])\n",
        "    cY1=int(keypoints['keypoints'][0][0][1])\n",
        "    cX=int(keypoints['keypoints'][0][1][0])\n",
        "    cY=int(keypoints['keypoints'][0][1][1])\n",
        "    im = Image.fromarray(np.uint8((image)*255))\n",
        "    image=np.asarray(im)\n",
        "    cv2.line(image, (cX, cY), (cX1, cY1), (256,0,0), 10)\n",
        "    cv2.circle(image, (cX, cY), 10, (255, 0, 0))\n",
        "    cv2.circle(image, (cX1, cY1), 10, (0, 255, 255))\n",
        "    plt.imshow(image)\n",
        "    \n",
        "  plt.show()\n"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YsIXutJ11PBN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#test for any images after importing the model \n",
        "image_test=(cv2.imread(\"image.address\"))#image you want to test\n",
        "image_test=T.ToPILImage()(image_test).convert('RGB')\n",
        "image_test= T.ToTensor()(image_test).to(torch.device('cuda:0'))\n",
        "image_test=[image_test]\n",
        "model.eval()\n",
        "out=model(image_test)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "show_result( zip(image_test,out))\n",
        "image_test[0].shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gPW6HnKswGt8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "torch.__version__"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7yWNV5lAPi5f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.onnx\n",
        "#torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
        "#Image=torch.tensor(Dataset[0][0].unsqueeze(dim=0),device=torch.device('cuda:0'))\n",
        "model.eval()\n",
        "out = model(image_test)\n",
        "dynamic_axes = {'input': [0, 2, 3], 'output': [0, 2, 3]}\n",
        "# Export the model\n",
        "torch.onnx.export(model,                     # model being run\n",
        "                  image_test,                # model input (or a tuple for multiple inputs)\n",
        "                  \"apple_axis.onnx\",         # where to save the model (can be a file or file-like object)\n",
        "                  export_params=True,        # store the trained parameter weights inside the model file\n",
        "                  opset_version=11,          # the ONNX version to export the model to\n",
        "                  do_constant_folding=True,  # whether to execute constant folding for optimization\n",
        "                  input_names = ['input'],   # the model's input names\n",
        "                  output_names = ['output'], # the model's output names\n",
        "                  dynamic_axes=dynamic_axes)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MreYFi9pBZQV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#setup for training\n",
        "num_classes = 2\n",
        "num_kpts=2\n",
        "# get the model using our helper function\n",
        "model, device, dtype=get_model(num_kpts,train_kptHead=2,train_fpn=True)\n",
        "# move model to the right device\n",
        "model.to(device)\n",
        "\n",
        "# construct an optimizer\n",
        "\n",
        "params1 = [p for p in model.roi_heads.keypoint_head.parameters() if p.requires_grad]\n",
        "\n",
        "params2 = [p for p in model.backbone.fpn.parameters() if p.requires_grad]\n",
        "optimizer = torch.optim.SGD([\n",
        "                {'params': params1},\n",
        "                {'params': model.roi_heads.keypoint_predictor.parameters(), 'lr': .1},\n",
        "                {'params': params2, 'lr': .001}\n",
        "            ], lr=.005, momentum=0.9)\n",
        "\"\"\"\n",
        "optim.SGD([\n",
        "                {'params': model.roi_heads.keypoint_head.parameters()},\n",
        "                {'params': model.roi_heads.keypoint_predictor.parameters(), 'lr': .1}\n",
        "            ], lr=.005, momentum=0.9)\n",
        "\"\"\"\n",
        "# and a learning rate scheduler which decreases the learning rate by\n",
        "# 5% every 3 epochs\n",
        "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\n",
        "                                               step_size=3,\n",
        "                                               gamma=0.95)\n",
        "print(params.__len__())\n",
        "for name,param in model.named_parameters():\n",
        "  if ( param.requires_grad ==True):\n",
        "    print(name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m52-bEQkCny8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#begin training\n",
        "from engine import train_one_epoch, evaluate\n",
        "\n",
        "num_epochs = 50\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    # train for one epoch, printing every 10 iterations\n",
        "    train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq=1)\n",
        "    # update the learning rate\n",
        "    lr_scheduler.step()\n",
        "    # evaluate on the test dataset\n",
        "    #evaluate(model, data_loader_test, device=device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s5vl3r9aabwq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#save model\n",
        "model_save_name = 'classifier4.pth'\n",
        "path = F\"/content/drive/My Drive/{model_save_name}\" \n",
        "torch.save(model.state_dict(), path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TeNMc9i93ClJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for names,param in model.named_parameters():\n",
        "  print (param.device)\n",
        "\n",
        "print(image_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NaycXf4B8vMY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# For Training\n",
        "images,targets = next(iter(data_loader))\n",
        "images = list(image for image in images)\n",
        "targets = [{k: v for k, v in t.items()} for t in targets]\n",
        "model.eval()\n",
        "output = model(images)   # Returns losses and detections\n",
        "output.__len__()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HOXwLZwFQDbE",
        "colab_type": "text"
      },
      "source": [
        "to download the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ehtWOvsQ1NOl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "from google.colab import files\n",
        "torch.save(model.state_dict(), 'checkpoint.pth')\n",
        "\n",
        "# download checkpoint file\n",
        "files.download('checkpoint.pth')\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}